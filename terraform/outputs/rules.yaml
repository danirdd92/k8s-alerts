apiVersion: 1
groups:
    - orgId: 1
      name: config-reloaders
      folder: alerts
      interval: 5m
      rules:
        - uid: ae5qm5of7ii9sc
          title: ConfigReloaderSidecarErrors
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: max_over_time(reloader_last_reload_successful{namespace=~".+"}[5m]) == 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: |-
                Errors encountered while the {{$labels.pod}} config-reloader sidecar attempts to sync config in {{$labels.namespace}} namespace.
                As a result, configuration for service running in {{$labels.pod}} may be stale and cannot be updated anymore.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/configreloadersidecarerrors
            summary: config-reloader sidecar has not had a successful reload for 10m
          labels:
            severity: warning
          isPaused: false
    - orgId: 1
      name: etcd
      folder: alerts
      interval: 5m
      rules:
        - uid: be5qm5pl3t728c
          title: etcdDatabaseHighFragmentationRatio
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (last_over_time(etcd_mvcc_db_total_size_in_use_in_bytes{job=~".*etcd.*"}[5m])
                    / last_over_time(etcd_mvcc_db_total_size_in_bytes{job=~".*etcd.*"}[5m])) < 0.5
                    and etcd_mvcc_db_total_size_in_use_in_bytes{job=~".*etcd.*"} > 104857600
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: 'etcd cluster "{{ $labels.job }}": database size in use on instance {{ $labels.instance }} is {{ $values.QUERY_RESULT.Value | humanizePercentage }} of the actual allocated disk space, please run defragmentation (e.g. etcdctl defrag) to retrieve the unused fragmented disk space.'
            runbook_url: https://etcd.io/docs/v3.5/op-guide/maintenance/#defragmentation
            summary: etcd database size in use is less than 50% of the actual allocated storage.
          labels:
            severity: warning
          isPaused: false
        - uid: ae5qm5poznif4a
          title: etcdDatabaseQuotaLowSpace
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: (last_over_time(etcd_mvcc_db_total_size_in_bytes{job=~".*etcd.*"}[5m]) / last_over_time(etcd_server_quota_backend_bytes{job=~".*etcd.*"}[5m]))*100 > 95
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: 'etcd cluster "{{ $labels.job }}": database size exceeds the defined quota on etcd instance {{ $labels.instance }}, please defrag or increase the quota as the writes to etcd will be disabled when it is full.'
            summary: etcd cluster database is running full.
          labels:
            severity: critical
          isPaused: false
        - uid: be5qm5poznif5f
          title: etcdExcessiveDatabaseGrowth
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: predict_linear(etcd_mvcc_db_total_size_in_bytes{job=~".*etcd.*"}[4h], 4*60*60) > etcd_server_quota_backend_bytes{job=~".*etcd.*"}
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: 'etcd cluster "{{ $labels.job }}": Predicting running out of disk space in the next four hours, based on write observations within the past four hours on etcd instance {{ $labels.instance }}, please check as it might be disruptive.'
            summary: etcd cluster database growing very fast.
          labels:
            severity: warning
          isPaused: false
        - uid: be5qm5pp25erkc
          title: etcdGRPCRequestsSlow
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    histogram_quantile(0.99, sum(rate(grpc_server_handling_seconds_bucket{job=~".*etcd.*", grpc_method!="Defragment", grpc_type="unary"}[5m])) without(grpc_type))
                    > 0.15
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: 'etcd cluster "{{ $labels.job }}": 99th percentile of gRPC requests is {{ $values.QUERY_RESULT.Value }}s on etcd instance {{ $labels.instance }} for {{ $labels.grpc_method }} method.'
            summary: etcd grpc requests are slow
          labels:
            severity: critical
          isPaused: false
        - uid: ee5qm5pp25erle
          title: etcdHighCommitDurations
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    histogram_quantile(0.99, rate(etcd_disk_backend_commit_duration_seconds_bucket{job=~".*etcd.*"}[5m]))
                    > 0.25
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: 'etcd cluster "{{ $labels.job }}": 99th percentile commit durations {{ $values.QUERY_RESULT.Value }}s on etcd instance {{ $labels.instance }}.'
            summary: etcd cluster 99th percentile commit durations are too high.
          labels:
            severity: warning
          isPaused: false
        - uid: de5qm5pp4nb40f
          title: etcdHighFsyncDurationsCritical
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~".*etcd.*"}[5m]))
                    > 1
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: 'etcd cluster "{{ $labels.job }}": 99th percentile fsync durations are {{ $values.QUERY_RESULT.Value }}s on etcd instance {{ $labels.instance }}.'
            summary: etcd cluster 99th percentile fsync durations are too high.
          labels:
            severity: critical
          isPaused: false
        - uid: ee5qm5pp4nb41d
          title: etcdHighFsyncDurationsWarn
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    histogram_quantile(0.99, rate(etcd_disk_wal_fsync_duration_seconds_bucket{job=~".*etcd.*"}[5m]))
                    > 0.5
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: 'etcd cluster "{{ $labels.job }}": 99th percentile fsync durations are {{ $values.QUERY_RESULT.Value }}s on etcd instance {{ $labels.instance }}.'
            summary: etcd cluster 99th percentile fsync durations are too high.
          labels:
            severity: warning
          isPaused: false
        - uid: ee5qm5pp4nb42b
          title: etcdHighNumberOfFailedGRPCRequests
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    100 * sum(rate(grpc_server_handled_total{job=~".*etcd.*", grpc_code=~"Unknown|FailedPrecondition|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded"}[5m])) without (grpc_type, grpc_code)
                      /
                    sum(rate(grpc_server_handled_total{job=~".*etcd.*"}[5m])) without (grpc_type, grpc_code)
                      > 5
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 5m
          annotations:
            description: 'etcd cluster "{{ $labels.job }}": {{ $values.QUERY_RESULT.Value }}% of requests for {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.'
            summary: etcd cluster has high number of failed grpc requests.
          labels:
            severity: critical
          isPaused: false
        - uid: ce5qm5pp757gge
          title: etcdHighNumberOfFailedGRPCRequestsWarn
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    100 * sum(rate(grpc_server_handled_total{job=~".*etcd.*", grpc_code=~"Unknown|FailedPrecondition|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded"}[5m])) without (grpc_type, grpc_code)
                      /
                    sum(rate(grpc_server_handled_total{job=~".*etcd.*"}[5m])) without (grpc_type, grpc_code)
                      > 1
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: 'etcd cluster "{{ $labels.job }}": {{ $values.QUERY_RESULT.Value }}% of requests for {{ $labels.grpc_method }} failed on etcd instance {{ $labels.instance }}.'
            summary: etcd cluster has high number of failed grpc requests.
          labels:
            severity: warning
          isPaused: false
        - uid: be5qm5pp757ghd
          title: etcdHighNumberOfFailedProposals
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: rate(etcd_server_proposals_failed_total{job=~".*etcd.*"}[15m]) > 5
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: 'etcd cluster "{{ $labels.job }}": {{ $values.QUERY_RESULT.Value }} proposal failures within the last 30 minutes on etcd instance {{ $labels.instance }}.'
            summary: etcd cluster has high number of proposal failures.
          labels:
            severity: warning
          isPaused: false
        - uid: fe5qm5pp9n3swd
          title: etcdHighNumberOfLeaderChanges
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    increase((max without (instance, pod) (etcd_server_leader_changes_seen_total{job=~".*etcd.*"})
                    or 0*absent(etcd_server_leader_changes_seen_total{job=~".*etcd.*"}))[15m:1m])
                    >= 4
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 5m
          annotations:
            description: 'etcd cluster "{{ $labels.job }}": {{ $values.QUERY_RESULT.Value }} leader changes within the last 15 minutes. Frequent elections may be a sign of insufficient resources, high network latency, or disruptions by other components and should be investigated.'
            summary: etcd cluster has high number of leader changes.
          labels:
            severity: warning
          isPaused: false
        - uid: be5qm5pp9n3sxc
          title: etcdInsufficientMembers
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: sum(up{job=~".*etcd.*"} == bool 1) without (instance, pod) < ((count(up{job=~".*etcd.*"}) without (instance, pod) + 1) / 2)
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 3m
          annotations:
            description: 'etcd cluster "{{ $labels.job }}": insufficient members ({{ $values.QUERY_RESULT.Value }}).'
            summary: etcd cluster has insufficient number of members.
          labels:
            severity: critical
          isPaused: false
        - uid: be5qm5pp9n3syc
          title: etcdMemberCommunicationSlow
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    histogram_quantile(0.99, rate(etcd_network_peer_round_trip_time_seconds_bucket{job=~".*etcd.*"}[5m]))
                    > 0.15
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: 'etcd cluster "{{ $labels.job }}": member communication with {{ $labels.To }} is taking {{ $values.QUERY_RESULT.Value }}s on etcd instance {{ $labels.instance }}.'
            summary: etcd cluster member communication is slow.
          labels:
            severity: warning
          isPaused: false
        - uid: de5qm5ppc505cb
          title: etcdMembersDown
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    max without (endpoint) (
                      sum without (instance, pod) (up{job=~".*etcd.*"} == bool 0)
                    or
                      count without (To) (
                        sum without (instance, pod) (rate(etcd_network_peer_sent_failures_total{job=~".*etcd.*"}[120s])) > 0.01
                      )
                    )
                    > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: 'etcd cluster "{{ $labels.job }}": members are down ({{ $values.QUERY_RESULT.Value }}).'
            summary: etcd cluster members are down.
          labels:
            severity: critical
          isPaused: false
        - uid: ae5qm5ppc505dd
          title: etcdNoLeader
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: etcd_server_has_leader{job=~".*etcd.*"} == 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 1m
          annotations:
            description: 'etcd cluster "{{ $labels.job }}": member {{ $labels.instance }} has no leader.'
            summary: etcd cluster has no leader.
          labels:
            severity: critical
          isPaused: false
    - orgId: 1
      name: general-rules
      folder: alerts
      interval: 5m
      rules:
        - uid: de5qm5ovauygwb
          title: InfoInhibitor
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: ALERTS{severity = "info"} == 1 unless on (namespace) ALERTS{alertname != "InfoInhibitor", severity =~ "warning|critical", alertstate="firing"} == 1
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          annotations:
            description: |
                This is an alert that is used to inhibit info alerts.
                By themselves, the info-level alerts are sometimes very noisy, but they are relevant when combined with
                other alerts.
                This alert fires whenever there's a severity="info" alert, and stops firing when another alert with a
                severity of 'warning' or 'critical' starts firing on the same namespace.
                This alert should be routed to a null receiver and configured to inhibit alerts with severity="info".
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/general/infoinhibitor
            summary: Info-level alert inhibition.
          labels:
            severity: none
          isPaused: false
        - uid: ae5qm5ovauygxd
          title: TargetDown
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: 100 * (count(up == 0) BY (cluster, job, namespace, service) / count(up) BY (cluster, job, namespace, service)) > 10
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: '{{ printf "%.4g" $values.QUERY_RESULT.Value }}% of the {{ $labels.job }}/{{ $labels.service }} targets in {{ $labels.namespace }} namespace are down.'
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/general/targetdown
            summary: One or more targets are unreachable.
          labels:
            severity: warning
          isPaused: false
        - uid: fe5qm5ovdcutcd
          title: Watchdog
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: vector(1)
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          annotations:
            description: |
                This is an alert meant to ensure that the entire alerting pipeline is functional.
                This alert is always firing, therefore it should always be firing in Alertmanager
                and always fire against a receiver. There are integrations with various notification
                mechanisms that send a notification when this alert is not firing. For example the
                "DeadMansSnitch" integration in PagerDuty.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/general/watchdog
            summary: An alert that should always be firing to certify that Alertmanager is working properly.
          labels:
            severity: none
          isPaused: false
    - orgId: 1
      name: kube-apiserver-slos
      folder: alerts
      interval: 5m
      rules:
        - uid: be5qm5r0kpvk0f
          title: KubeAPIErrorBudgetBurn15Min
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    sum by (cluster) (apiserver_request:burnrate6h) > (6.00 * 0.01000)
                    and on (cluster)
                    sum by (cluster) (apiserver_request:burnrate30m) > (6.00 * 0.01000)
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: The API server is burning too much error budget.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn
            summary: The API server is burning too much error budget.
          labels:
            long: 6h
            severity: critical
            short: 30m
          isPaused: false
        - uid: be5qm5r2xmfb4e
          title: KubeAPIErrorBudgetBurn1Hour
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    sum by (cluster) (apiserver_request:burnrate1d) > (3.00 * 0.01000)
                    and on (cluster)
                    sum by (cluster) (apiserver_request:burnrate2h) > (3.00 * 0.01000)
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 1h
          annotations:
            description: The API server is burning too much error budget.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn
            summary: The API server is burning too much error budget.
          labels:
            long: 1d
            severity: warning
            short: 2h
          isPaused: false
        - uid: de5qm5r304bnka
          title: KubeAPIErrorBudgetBurn2Min
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    sum by (cluster) (apiserver_request:burnrate1h) > (14.40 * 0.01000)
                    and on (cluster)
                    sum by (cluster) (apiserver_request:burnrate5m) > (14.40 * 0.01000)
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 2m
          annotations:
            description: The API server is burning too much error budget.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn
            summary: The API server is burning too much error budget.
          labels:
            long: 1h
            severity: critical
            short: 5m
          isPaused: false
        - uid: de5qm5r304bnlb
          title: KubeAPIErrorBudgetBurn3Hour
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    sum by (cluster) (apiserver_request:burnrate3d) > (1.00 * 0.01000)
                    and on (cluster)
                    sum by (cluster) (apiserver_request:burnrate6h) > (1.00 * 0.01000)
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 3h
          annotations:
            description: The API server is burning too much error budget.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapierrorbudgetburn
            summary: The API server is burning too much error budget.
          labels:
            long: 3d
            severity: warning
            short: 6h
          isPaused: false
    - orgId: 1
      name: kube-state-metrics
      folder: alerts
      interval: 5m
      rules:
        - uid: de5qm5wokxc74e
          title: KubeStateMetricsListErrors
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (sum(rate(kube_state_metrics_list_total{job="kube-state-metrics",result="error"}[5m])) by (cluster)
                      /
                    sum(rate(kube_state_metrics_list_total{job="kube-state-metrics"}[5m])) by (cluster))
                    > 0.01
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: kube-state-metrics is experiencing errors at an elevated rate in list operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricslisterrors
            summary: kube-state-metrics is experiencing errors in list operations.
          labels:
            severity: critical
          isPaused: false
        - uid: ce5qm5wosf18ga
          title: KubeStateMetricsShardingMismatch
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: stdvar (kube_state_metrics_total_shards{job="kube-state-metrics"}) by (cluster) != 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: kube-state-metrics pods are running with different --total-shards configuration, some Kubernetes objects may be exposed multiple times or not exposed at all.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricsshardingmismatch
            summary: kube-state-metrics sharding is misconfigured.
          labels:
            severity: critical
          isPaused: false
        - uid: ce5qm5woxetxcf
          title: KubeStateMetricsShardsMissing
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    2^max(kube_state_metrics_total_shards{job="kube-state-metrics"}) by (cluster) - 1
                      -
                    sum( 2 ^ max by (cluster, shard_ordinal) (kube_state_metrics_shard_ordinal{job="kube-state-metrics"}) ) by (cluster)
                    != 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: kube-state-metrics shards are missing, some Kubernetes objects are not being exposed.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricsshardsmissing
            summary: kube-state-metrics shards are missing.
          labels:
            severity: critical
          isPaused: false
        - uid: de5qm5woxetxdc
          title: KubeStateMetricsWatchErrors
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics",result="error"}[5m])) by (cluster)
                      /
                    sum(rate(kube_state_metrics_watch_total{job="kube-state-metrics"}[5m])) by (cluster))
                    > 0.01
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: kube-state-metrics is experiencing errors at an elevated rate in watch operations. This is likely causing it to not be able to expose metrics about Kubernetes objects correctly or at all.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kube-state-metrics/kubestatemetricswatcherrors
            summary: kube-state-metrics is experiencing errors in watch operations.
          labels:
            severity: critical
          isPaused: false
    - orgId: 1
      name: kubernetes-apps
      folder: alerts
      interval: 5m
      rules:
        - uid: ae5qm5q441ds0d
          title: KubeContainerWaiting
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: kube_pod_container_status_waiting_reason{reason!="CrashLoopBackOff", job="kube-state-metrics", namespace=~".*"} > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 1h
          annotations:
            description: 'pod/{{ $labels.pod }} in namespace {{ $labels.namespace }} on container {{ $labels.container}} has been in waiting state for longer than 1 hour. (reason: "{{ $labels.reason }}").'
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontainerwaiting
            summary: Pod container waiting longer than 1 hour
          labels:
            severity: warning
          isPaused: false
        - uid: ce5qm5qbj8irkc
          title: KubeDaemonSetMisScheduled
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: kube_daemonset_status_number_misscheduled{job="kube-state-metrics", namespace=~".*"} > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: '{{ $values.QUERY_RESULT.Value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are running where they are not supposed to run.'
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetmisscheduled
            summary: DaemonSet pods are misscheduled.
          labels:
            severity: warning
          isPaused: false
        - uid: de5qm5qbo8bggd
          title: KubeDaemonSetNotScheduled
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
                      -
                    kube_daemonset_status_current_number_scheduled{job="kube-state-metrics", namespace=~".*"} > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: '{{ $values.QUERY_RESULT.Value }} Pods of DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} are not scheduled.'
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetnotscheduled
            summary: DaemonSet pods are not scheduled.
          labels:
            severity: warning
          isPaused: false
        - uid: be5qm5qby7wu8e
          title: KubeDaemonSetRolloutStuck
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      (
                        kube_daemonset_status_current_number_scheduled{job="kube-state-metrics", namespace=~".*"}
                         !=
                        kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
                      ) or (
                        kube_daemonset_status_number_misscheduled{job="kube-state-metrics", namespace=~".*"}
                         !=
                        0
                      ) or (
                        kube_daemonset_status_updated_number_scheduled{job="kube-state-metrics", namespace=~".*"}
                         !=
                        kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
                      ) or (
                        kube_daemonset_status_number_available{job="kube-state-metrics", namespace=~".*"}
                         !=
                        kube_daemonset_status_desired_number_scheduled{job="kube-state-metrics", namespace=~".*"}
                      )
                    ) and (
                      changes(kube_daemonset_status_updated_number_scheduled{job="kube-state-metrics", namespace=~".*"}[5m])
                        ==
                      0
                    )
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} has not finished or progressed for at least 15 minutes.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedaemonsetrolloutstuck
            summary: DaemonSet rollout is stuck.
          labels:
            severity: warning
          isPaused: false
        - uid: be5qm5qc87i80a
          title: KubeDeploymentGenerationMismatch
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    kube_deployment_status_observed_generation{job="kube-state-metrics", namespace=~".*"}
                      !=
                    kube_deployment_metadata_generation{job="kube-state-metrics", namespace=~".*"}
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Deployment generation for {{ $labels.namespace }}/{{ $labels.deployment }} does not match, this indicates that the Deployment has failed but has not been rolled back.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentgenerationmismatch
            summary: Deployment generation mismatch due to possible roll-back
          labels:
            severity: warning
          isPaused: false
        - uid: ce5qm5qci73lsf
          title: KubeDeploymentReplicasMismatch
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      kube_deployment_spec_replicas{job="kube-state-metrics", namespace=~".*"}
                        >
                      kube_deployment_status_replicas_available{job="kube-state-metrics", namespace=~".*"}
                    ) and (
                      changes(kube_deployment_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}[10m])
                        ==
                      0
                    )
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than 15 minutes.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentreplicasmismatch
            summary: Deployment has not matched the expected number of replicas.
          labels:
            severity: warning
          isPaused: false
        - uid: fe5qm5qci73ltb
          title: KubeDeploymentRolloutStuck
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    kube_deployment_status_condition{condition="Progressing", status="false",job="kube-state-metrics", namespace=~".*"}
                    != 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Rollout of deployment {{ $labels.namespace }}/{{ $labels.deployment }} is not progressing for longer than 15 minutes.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentrolloutstuck
            summary: Deployment rollout is not progressing.
          labels:
            severity: warning
          isPaused: false
        - uid: fe5qm5qckozy8c
          title: KubeHpaMaxedOut
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"}
                      ==
                    kube_horizontalpodautoscaler_spec_max_replicas{job="kube-state-metrics", namespace=~".*"}
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has been running at max replicas for longer than 15 minutes.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubehpamaxedout
            summary: HPA is running at max replicas
          labels:
            severity: warning
          isPaused: false
        - uid: de5qm5qckozy9f
          title: KubeHpaReplicasMismatch
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (kube_horizontalpodautoscaler_status_desired_replicas{job="kube-state-metrics", namespace=~".*"}
                      !=
                    kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"})
                      and
                    (kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"}
                      >
                    kube_horizontalpodautoscaler_spec_min_replicas{job="kube-state-metrics", namespace=~".*"})
                      and
                    (kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"}
                      <
                    kube_horizontalpodautoscaler_spec_max_replicas{job="kube-state-metrics", namespace=~".*"})
                      and
                    changes(kube_horizontalpodautoscaler_status_current_replicas{job="kube-state-metrics", namespace=~".*"}[15m]) == 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler  }} has not matched the desired number of replicas for longer than 15 minutes.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubehpareplicasmismatch
            summary: HPA has not matched desired number of replicas.
          labels:
            severity: warning
          isPaused: false
        - uid: ee5qm5qcn6waoa
          title: KubeJobFailed
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: kube_job_failed{job="kube-state-metrics", namespace=~".*"}  > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Job {{ $labels.namespace }}/{{ $labels.job_name }} failed to complete. Removing failed job after investigation should clear this alert.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobfailed
            summary: Job failed to complete.
          labels:
            severity: warning
          isPaused: false
        - uid: ae5qm5qcuolc0d
          title: KubeJobNotCompleted
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    time() - max by (namespace, job_name, cluster) (kube_job_status_start_time{job="kube-state-metrics", namespace=~".*"}
                      and
                    kube_job_status_active{job="kube-state-metrics", namespace=~".*"} > 0) > 43200
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          annotations:
            description: Job {{ $labels.namespace }}/{{ $labels.job_name }} is taking more than {{ "43200" | humanizeDuration }} to complete.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubejobnotcompleted
            summary: Job did not complete in time
          labels:
            severity: warning
          isPaused: false
        - uid: be5qm5qcx6hoge
          title: KubePodCrashLooping
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: max_over_time(kube_pod_container_status_waiting_reason{reason="CrashLoopBackOff",job="kube-state-metrics", namespace=~".*"}[5m]) >= 1
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: 'Pod {{ $labels.namespace }}/{{ $labels.pod }} ({{ $labels.container }}) is in waiting state (reason: "CrashLoopBackOff").'
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodcrashlooping
            summary: Pod is crash looping.
          labels:
            severity: warning
          isPaused: false
        - uid: ce5qm5qczoe0wa
          title: KubePodNotReady
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    sum by (namespace, pod, cluster) (
                      max by (namespace, pod, cluster) (
                        kube_pod_status_phase{job="kube-state-metrics", namespace=~".*", phase=~"Pending|Unknown|Failed"}
                      ) * on (namespace, pod, cluster) group_left(owner_kind) topk by (namespace, pod, cluster) (
                        1, max by (namespace, pod, owner_kind, cluster) (kube_pod_owner{owner_kind!="Job"})
                      )
                    ) > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 15 minutes.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodnotready
            summary: Pod has been in a non-ready state for more than 15 minutes.
          labels:
            severity: warning
          isPaused: false
        - uid: ee5qm5qdc5vr4d
          title: KubeStatefulSetGenerationMismatch
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    kube_statefulset_status_observed_generation{job="kube-state-metrics", namespace=~".*"}
                      !=
                    kube_statefulset_metadata_generation{job="kube-state-metrics", namespace=~".*"}
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: StatefulSet generation for {{ $labels.namespace }}/{{ $labels.statefulset }} does not match, this indicates that the StatefulSet has failed but has not been rolled back.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetgenerationmismatch
            summary: StatefulSet generation mismatch due to possible roll-back
          labels:
            severity: warning
          isPaused: false
        - uid: ee5qm5qdens3kc
          title: KubeStatefulSetReplicasMismatch
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      kube_statefulset_status_replicas_ready{job="kube-state-metrics", namespace=~".*"}
                        !=
                      kube_statefulset_status_replicas{job="kube-state-metrics", namespace=~".*"}
                    ) and (
                      changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}[10m])
                        ==
                      0
                    )
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} has not matched the expected number of replicas for longer than 15 minutes.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetreplicasmismatch
            summary: StatefulSet has not matched the expected number of replicas.
          labels:
            severity: warning
          isPaused: false
        - uid: ee5qm5qdh5og0c
          title: KubeStatefulSetUpdateNotRolledOut
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      max by (namespace, statefulset, job, cluster) (
                        kube_statefulset_status_current_revision{job="kube-state-metrics", namespace=~".*"}
                          unless
                        kube_statefulset_status_update_revision{job="kube-state-metrics", namespace=~".*"}
                      )
                        *
                      (
                        kube_statefulset_replicas{job="kube-state-metrics", namespace=~".*"}
                          !=
                        kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}
                      )
                    )  and (
                      changes(kube_statefulset_status_replicas_updated{job="kube-state-metrics", namespace=~".*"}[5m])
                        ==
                      0
                    )
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} update has not been rolled out.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubestatefulsetupdatenotrolledout
            summary: StatefulSet update has not been rolled out.
          labels:
            severity: warning
          isPaused: false
    - orgId: 1
      name: kubernetes-resources
      folder: alerts
      interval: 5m
      rules:
        - uid: ce5qm5vi9n9c0d
          title: CPUThrottlingHigh
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    sum(increase(container_cpu_cfs_throttled_periods_total{container!="", job="kubelet", metrics_path="/metrics/cadvisor", }[5m])) without (id, metrics_path, name, image, endpoint, job, node)
                      /
                    sum(increase(container_cpu_cfs_periods_total{job="kubelet", metrics_path="/metrics/cadvisor", }[5m])) without (id, metrics_path, name, image, endpoint, job, node)
                      > ( 25 / 100 )
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: '{{ $values.QUERY_RESULT.Value | humanizePercentage }} throttling of CPU in namespace {{ $labels.namespace }} for container {{ $labels.container }} in pod {{ $labels.pod }}.'
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/cputhrottlinghigh
            summary: Processes experience elevated CPU throttling.
          labels:
            severity: info
          isPaused: false
        - uid: be5qm5vjl3bwga
          title: KubeCPUOvercommit
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    sum(namespace_cpu:kube_pod_container_resource_requests:sum{}) by (cluster) - (sum(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster) - max(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster)) > 0
                    and
                    (sum(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster) - max(kube_node_status_allocatable{job="kube-state-metrics",resource="cpu"}) by (cluster)) > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: Cluster {{ $labels.cluster }} has overcommitted CPU resource requests for Pods by {{ $values.QUERY_RESULT.Value }} CPU shares and cannot tolerate node failure.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecpuovercommit
            summary: Cluster has overcommitted CPU resource requests.
          labels:
            severity: warning
          isPaused: false
        - uid: ae5qm5vjnl88we
          title: KubeCPUQuotaOvercommit
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    sum(min without(resource) (kube_resourcequota{job="kube-state-metrics", type="hard", resource=~"(cpu|requests.cpu)"})) by (cluster)
                      /
                    sum(kube_node_status_allocatable{resource="cpu", job="kube-state-metrics"}) by (cluster)
                      > 1.5
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 5m
          annotations:
            description: Cluster {{ $labels.cluster }}  has overcommitted CPU resource requests for Namespaces.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecpuquotaovercommit
            summary: Cluster has overcommitted CPU resource requests.
          labels:
            severity: warning
          isPaused: false
        - uid: be5qm5vjnl88xe
          title: KubeMemoryOvercommit
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    sum(namespace_memory:kube_pod_container_resource_requests:sum{}) by (cluster) - (sum(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster) - max(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster)) > 0
                    and
                    (sum(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster) - max(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster)) > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: Cluster {{ $labels.cluster }} has overcommitted memory resource requests for Pods by {{ $values.QUERY_RESULT.Value | humanize }} bytes and cannot tolerate node failure.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubememoryovercommit
            summary: Cluster has overcommitted memory resource requests.
          labels:
            severity: warning
          isPaused: false
        - uid: be5qm5vjnl88ye
          title: KubeMemoryQuotaOvercommit
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    sum(min without(resource) (kube_resourcequota{job="kube-state-metrics", type="hard", resource=~"(memory|requests.memory)"})) by (cluster)
                      /
                    sum(kube_node_status_allocatable{resource="memory", job="kube-state-metrics"}) by (cluster)
                      > 1.5
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 5m
          annotations:
            description: Cluster {{ $labels.cluster }}  has overcommitted memory resource requests for Namespaces.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubememoryquotaovercommit
            summary: Cluster has overcommitted memory resource requests.
          labels:
            severity: warning
          isPaused: false
        - uid: be5qm5vjnl88zb
          title: KubeQuotaAlmostFull
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    kube_resourcequota{job="kube-state-metrics", type="used"}
                      / ignoring(instance, job, type)
                    (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
                      > 0.9 < 1
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Namespace {{ $labels.namespace }} is using {{ $values.QUERY_RESULT.Value | humanizePercentage }} of its {{ $labels.resource }} quota.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotaalmostfull
            summary: Namespace quota is going to be full.
          labels:
            severity: info
          isPaused: false
        - uid: ce5qm5vjnl890c
          title: KubeQuotaExceeded
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    kube_resourcequota{job="kube-state-metrics", type="used"}
                      / ignoring(instance, job, type)
                    (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
                      > 1
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Namespace {{ $labels.namespace }} is using {{ $values.QUERY_RESULT.Value | humanizePercentage }} of its {{ $labels.resource }} quota.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotaexceeded
            summary: Namespace quota has exceeded the limits.
          labels:
            severity: warning
          isPaused: false
        - uid: fe5qm5vjq34lcb
          title: KubeQuotaFullyUsed
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    kube_resourcequota{job="kube-state-metrics", type="used"}
                      / ignoring(instance, job, type)
                    (kube_resourcequota{job="kube-state-metrics", type="hard"} > 0)
                      == 1
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Namespace {{ $labels.namespace }} is using {{ $values.QUERY_RESULT.Value | humanizePercentage }} of its {{ $labels.resource }} quota.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubequotafullyused
            summary: Namespace quota is fully used.
          labels:
            severity: info
          isPaused: false
    - orgId: 1
      name: kubernetes-storage
      folder: alerts
      interval: 5m
      rules:
        - uid: ae5qm5vgy76rkf
          title: KubePersistentVolumeErrors
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: kube_persistentvolume_status_phase{phase=~"Failed|Pending",job="kube-state-metrics"} > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 5m
          annotations:
            description: The persistent volume {{ $labels.persistentvolume }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} has status {{ $labels.phase }}.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeerrors
            summary: PersistentVolume is having issues with provisioning.
          labels:
            severity: critical
          isPaused: false
        - uid: de5qm5vh0p341e
          title: KubePersistentVolumeFillingUp
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      kubelet_volume_stats_available_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
                        /
                      kubelet_volume_stats_capacity_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
                    ) < 0.03
                    and
                    kubelet_volume_stats_used_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"} > 0
                    unless on (cluster, namespace, persistentvolumeclaim)
                    kube_persistentvolumeclaim_access_mode{ access_mode="ReadOnlyMany"} == 1
                    unless on (cluster, namespace, persistentvolumeclaim)
                    kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 1m
          annotations:
            description: |-
                The PersistentVolume claimed by {{ $labels.persistentvolumeclaim
                }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster
                {{ . }} {{- end }} is only {{ $values.QUERY_RESULT.Value | humanizePercentage }} free.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumefillingup
            summary: PersistentVolume is filling up.
          labels:
            severity: critical
          isPaused: false
        - uid: ce5qm5vh36zgge
          title: KubePersistentVolumeFillingUpWithinDays
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      kubelet_volume_stats_available_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
                        /
                      kubelet_volume_stats_capacity_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
                    ) < 0.15
                    and
                    kubelet_volume_stats_used_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"} > 0
                    and
                    predict_linear(kubelet_volume_stats_available_bytes{job="kubelet", namespace=~".*", metrics_path="/metrics"}[6h], 4 * 24 * 3600) < 0
                    unless on (cluster, namespace, persistentvolumeclaim)
                    kube_persistentvolumeclaim_access_mode{ access_mode="ReadOnlyMany"} == 1
                    unless on (cluster, namespace, persistentvolumeclaim)
                    kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 1h
          annotations:
            description: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} is expected to fill up within four days. Currently {{ $values.QUERY_RESULT.Value | humanizePercentage }} is available.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumefillingup
            summary: PersistentVolume is filling up.
          labels:
            severity: warning
          isPaused: false
        - uid: fe5qm5vh36zghd
          title: KubePersistentVolumeInodesFillingUp
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      kubelet_volume_stats_inodes_free{job="kubelet", namespace=~".*", metrics_path="/metrics"}
                        /
                      kubelet_volume_stats_inodes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
                    ) < 0.03
                    and
                    kubelet_volume_stats_inodes_used{job="kubelet", namespace=~".*", metrics_path="/metrics"} > 0
                    unless on (cluster, namespace, persistentvolumeclaim)
                    kube_persistentvolumeclaim_access_mode{ access_mode="ReadOnlyMany"} == 1
                    unless on (cluster, namespace, persistentvolumeclaim)
                    kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 1m
          annotations:
            description: The PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} only has {{ $values.QUERY_RESULT.Value | humanizePercentage }} free inodes.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeinodesfillingup
            summary: PersistentVolumeInodes are filling up.
          labels:
            severity: critical
          isPaused: false
        - uid: ee5qm5vh5ovsxc
          title: KubePersistentVolumeInodesFillingUpWithinDays
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      kubelet_volume_stats_inodes_free{job="kubelet", namespace=~".*", metrics_path="/metrics"}
                        /
                      kubelet_volume_stats_inodes{job="kubelet", namespace=~".*", metrics_path="/metrics"}
                    ) < 0.15
                    and
                    kubelet_volume_stats_inodes_used{job="kubelet", namespace=~".*", metrics_path="/metrics"} > 0
                    and
                    predict_linear(kubelet_volume_stats_inodes_free{job="kubelet", namespace=~".*", metrics_path="/metrics"}[6h], 4 * 24 * 3600) < 0
                    unless on (cluster, namespace, persistentvolumeclaim)
                    kube_persistentvolumeclaim_access_mode{ access_mode="ReadOnlyMany"} == 1
                    unless on (cluster, namespace, persistentvolumeclaim)
                    kube_persistentvolumeclaim_labels{label_excluded_from_alerts="true"} == 1
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 1h
          annotations:
            description: Based on recent sampling, the PersistentVolume claimed by {{ $labels.persistentvolumeclaim }} in Namespace {{ $labels.namespace }} {{ with $labels.cluster -}} on Cluster {{ . }} {{- end }} is expected to run out of inodes within four days. Currently {{ $values.QUERY_RESULT.Value | humanizePercentage }} of its inodes are free.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepersistentvolumeinodesfillingup
            summary: PersistentVolumeInodes are filling up.
          labels:
            severity: warning
          isPaused: false
    - orgId: 1
      name: kubernetes-system
      folder: alerts
      interval: 5m
      rules:
        - uid: ce5qm5ucdckjkc
          title: KubeClientErrors
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (sum(rate(rest_client_requests_total{job="apiserver",code=~"5.."}[5m])) by (cluster, instance, job, namespace)
                      /
                    sum(rate(rest_client_requests_total{job="apiserver"}[5m])) by (cluster, instance, job, namespace))
                    > 0.01
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Kubernetes API server client '{{ $labels.job }}/{{ $labels.instance }}' is experiencing {{ $values.QUERY_RESULT.Value | humanizePercentage }} errors.'
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclienterrors
            summary: Kubernetes API server client is experiencing errors.
          labels:
            severity: warning
          isPaused: false
        - uid: ae5qm5ucfugw0f
          title: KubeVersionMismatch
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: count by (cluster) (count by (git_version, cluster) (label_replace(kubernetes_build_info{job!~"kube-dns|coredns"},"git_version","$1","git_version","(v[0-9]*.[0-9]*).*"))) > 1
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: There are {{ $values.QUERY_RESULT.Value }} different semantic versions of Kubernetes components running.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeversionmismatch
            summary: Different semantic versions of Kubernetes components running.
          labels:
            severity: warning
          isPaused: false
    - orgId: 1
      name: kubernetes-system-apiserver
      folder: alerts
      interval: 5m
      rules:
        - uid: ce5qm5v3rqh34d
          title: KubeAPIDown
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: absent(up{job="apiserver"} == 1)
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: KubeAPI has disappeared from Prometheus target discovery.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapidown
            summary: Target disappeared from Prometheus target discovery.
          labels:
            severity: critical
          isPaused: false
        - uid: ee5qm5v3u8dfke
          title: KubeAPITerminatedRequests
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    sum by (cluster) (rate(apiserver_request_terminations_total{job="apiserver"}[10m]))
                    / ( sum by (cluster) (rate(apiserver_request_total{job="apiserver"}[10m])) +
                    sum by (cluster) (rate(apiserver_request_terminations_total{job="apiserver"}[10m]))
                    ) > 0.20
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 5m
          annotations:
            description: The kubernetes apiserver has terminated {{ $values.QUERY_RESULT.Value | humanizePercentage }} of its incoming requests.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeapiterminatedrequests
            summary: The kubernetes apiserver has terminated {{ $values.QUERY_RESULT.Value | humanizePercentage }} of its incoming requests.
          labels:
            severity: warning
          isPaused: false
        - uid: ae5qm5v3z864gd
          title: KubeAggregatedAPIDown
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: (1 - max by (name, namespace, cluster)(avg_over_time(aggregator_unavailable_apiservice{job="apiserver"}[10m]))) * 100 < 85
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 5m
          annotations:
            description: Kubernetes aggregated API {{ $labels.name }}/{{ $labels.namespace }} has been only {{ $values.QUERY_RESULT.Value | humanize }}% available over the last 10m.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeaggregatedapidown
            summary: Kubernetes aggregated API is down.
          labels:
            severity: warning
          isPaused: false
        - uid: be5qm5v3z864hf
          title: KubeAggregatedAPIErrors
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: sum by (name, namespace, cluster)(increase(aggregator_unavailable_apiservice_total{job="apiserver"}[10m])) > 4
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          annotations:
            description: Kubernetes aggregated API {{ $labels.name }}/{{ $labels.namespace }} has reported errors. It has appeared unavailable {{ $values.QUERY_RESULT.Value | humanize }} times averaged over the past 10m.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeaggregatedapierrors
            summary: Kubernetes aggregated API has reported errors.
          labels:
            severity: warning
          isPaused: false
        - uid: de5qm5v41q2gwf
          title: KubeClientCertificateExpirationIn24Hours
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    histogram_quantile(0.01, sum without (namespace, service, endpoint) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 86400
                    and
                    on (job, cluster, instance) apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 5m
          annotations:
            description: A client certificate used to authenticate to kubernetes apiserver is expiring in less than 24.0 hours.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclientcertificateexpiration
            summary: Client certificate is about to expire.
          labels:
            severity: critical
          isPaused: false
        - uid: ae5qm5v447ytcd
          title: KubeClientCertificateExpirationInWeek
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    histogram_quantile(0.01, sum without (namespace, service, endpoint) (rate(apiserver_client_certificate_expiration_seconds_bucket{job="apiserver"}[5m]))) < 604800
                    and
                    on (job, cluster, instance) apiserver_client_certificate_expiration_seconds_count{job="apiserver"} > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 5m
          annotations:
            description: A client certificate used to authenticate to kubernetes apiserver is expiring in less than 7.0 days.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeclientcertificateexpiration
            summary: Client certificate is about to expire.
          labels:
            severity: warning
          isPaused: false
    - orgId: 1
      name: kubernetes-system-controller-manager
      folder: alerts
      interval: 5m
      rules:
        - uid: ae5qm5ohwwjr4f
          title: KubeControllerManagerDown
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: absent(up{job="kube-controller-manager"} == 1)
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: KubeControllerManager has disappeared from Prometheus target discovery.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontrollermanagerdown
            summary: Target disappeared from Prometheus target discovery.
          labels:
            severity: critical
          isPaused: false
    - orgId: 1
      name: kubernetes-system-kube-proxy
      folder: alerts
      interval: 5m
      rules:
        - uid: be5qm5rkrec5cb
          title: KubeProxyDown
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: absent(up{job="kube-proxy"} == 1)
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: KubeProxy has disappeared from Prometheus target discovery.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeproxydown
            summary: Target disappeared from Prometheus target discovery.
          labels:
            severity: critical
          isPaused: false
    - orgId: 1
      name: kubernetes-system-kubelet
      folder: alerts
      interval: 5m
      rules:
        - uid: ee5qm5qon7c3kb
          title: KubeNodeNotReady
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: kube_node_status_condition{job="kube-state-metrics",condition="Ready",status="true"} == 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: '{{ $labels.node }} has been unready for more than 15 minutes.'
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodenotready
            summary: Node is not ready.
          labels:
            severity: warning
          isPaused: false
        - uid: de5qm5qon7c3le
          title: KubeNodeReadinessFlapping
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: sum(changes(kube_node_status_condition{job="kube-state-metrics",status="true",condition="Ready"}[15m])) by (cluster, node) > 2
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: The readiness status of node {{ $labels.node }} has changed {{ $values.QUERY_RESULT.Value }} times in the last 15 minutes.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodereadinessflapping
            summary: Node readiness status is flapping.
          labels:
            severity: warning
          isPaused: false
        - uid: de5qm5qopp8g0a
          title: KubeNodeUnreachable
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (kube_node_spec_taint{job="kube-state-metrics",key="node.kubernetes.io/unreachable",effect="NoSchedule"}
                    unless ignoring(key,value) kube_node_spec_taint{job="kube-state-metrics",key=~"ToBeDeletedByClusterAutoscaler|cloud.google.com/impending-node-termination|aws-node-termination-handler/spot-itn"})
                    == 1
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: '{{ $labels.node }} is unreachable and some workloads may be rescheduled.'
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubenodeunreachable
            summary: Node is unreachable.
          labels:
            severity: warning
          isPaused: false
        - uid: be5qm5qopp8g1f
          title: KubeletClientCertificateExpirationTommorow
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: kubelet_certificate_manager_client_ttl_seconds < 86400
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          annotations:
            description: Client certificate for Kubelet on node {{ $labels.node }} expires in {{ $values.QUERY_RESULT.Value | humanizeDuration }}.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificateexpiration
            summary: Kubelet client certificate is about to expire.
          labels:
            severity: critical
          isPaused: false
        - uid: be5qm5qopp8g2c
          title: KubeletClientCertificateExpirationWithinWeek
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: kubelet_certificate_manager_client_ttl_seconds < 604800
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          annotations:
            description: Client certificate for Kubelet on node {{ $labels.node }} expires in {{ $values.QUERY_RESULT.Value | humanizeDuration }}.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificateexpiration
            summary: Kubelet client certificate is about to expire.
          labels:
            severity: warning
          isPaused: false
        - uid: ce5qm5qopp8g3d
          title: KubeletClientCertificateRenewalErrors
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: increase(kubelet_certificate_manager_client_expiration_renew_errors[5m]) > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Kubelet on node {{ $labels.node }} has failed to renew its client certificate ({{ $values.QUERY_RESULT.Value | humanize }} errors in the last 5 minutes).
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletclientcertificaterenewalerrors
            summary: Kubelet has failed to renew its client certificate.
          labels:
            severity: warning
          isPaused: false
        - uid: de5qm5qos74sgc
          title: KubeletDown
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: absent(up{job="kubelet", metrics_path="/metrics"} == 1)
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Kubelet has disappeared from Prometheus target discovery.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletdown
            summary: Target disappeared from Prometheus target discovery.
          labels:
            severity: critical
          isPaused: false
        - uid: be5qm5qos74sha
          title: KubeletPlegDurationHigh
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: node_quantile:kubelet_pleg_relist_duration_seconds:histogram_quantile{quantile="0.99"} >= 10
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 5m
          annotations:
            description: The Kubelet Pod Lifecycle Event Generator has a 99th percentile duration of {{ $values.QUERY_RESULT.Value }} seconds on node {{ $labels.node }}.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletplegdurationhigh
            summary: Kubelet Pod Lifecycle Event Generator is taking too long to relist.
          labels:
            severity: warning
          isPaused: false
        - uid: de5qm5qos74sib
          title: KubeletPodStartUpLatencyHigh
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    histogram_quantile(0.99, sum(rate(kubelet_pod_worker_duration_seconds_bucket{job="kubelet",
                    metrics_path="/metrics"}[5m])) by (cluster, instance, le)) * on (cluster, instance)
                    group_left(node) kubelet_node_name{job="kubelet", metrics_path="/metrics"} >
                    60
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Kubelet Pod startup 99th percentile latency is {{ $values.QUERY_RESULT.Value }} seconds on node {{ $labels.node }}.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletpodstartuplatencyhigh
            summary: Kubelet Pod startup latency is too high.
          labels:
            severity: warning
          isPaused: false
        - uid: ae5qm5qos74sja
          title: KubeletServerCertificateExpirationTommorow
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: kubelet_certificate_manager_server_ttl_seconds < 86400
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          annotations:
            description: Server certificate for Kubelet on node {{ $labels.node }} expires in {{ $values.QUERY_RESULT.Value | humanizeDuration }}.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificateexpiration
            summary: Kubelet server certificate is about to expire.
          labels:
            severity: critical
          isPaused: false
        - uid: ee5qm5qoup14wb
          title: KubeletServerCertificateExpirationWithinWeek
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: kubelet_certificate_manager_server_ttl_seconds < 604800
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          annotations:
            description: Server certificate for Kubelet on node {{ $labels.node }} expires in {{ $values.QUERY_RESULT.Value | humanizeDuration }}.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificateexpiration
            summary: Kubelet server certificate is about to expire.
          labels:
            severity: warning
          isPaused: false
        - uid: ae5qm5qoup14xf
          title: KubeletServerCertificateRenewalErrors
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: increase(kubelet_server_expiration_renew_errors[5m]) > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Kubelet on node {{ $labels.node }} has failed to renew its server certificate ({{ $values.QUERY_RESULT.Value | humanize }} errors in the last 5 minutes).
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeletservercertificaterenewalerrors
            summary: Kubelet has failed to renew its server certificate.
          labels:
            severity: warning
          isPaused: false
        - uid: de5qm5qozotttd
          title: KubeletTooManyPods
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    count by (cluster, node) (
                      (kube_pod_status_phase{job="kube-state-metrics",phase="Running"} == 1) * on (instance,pod,namespace,cluster) group_left(node) topk by (instance,pod,namespace,cluster) (1, kube_pod_info{job="kube-state-metrics"})
                    )
                    /
                    max by (cluster, node) (
                      kube_node_status_capacity{job="kube-state-metrics",resource="pods"} != 1
                    ) > 0.95
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Kubelet '{{ $labels.node }}' is running at {{ $values.QUERY_RESULT.Value | humanizePercentage }} of its Pod capacity.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubelettoomanypods
            summary: Kubelet is running at capacity.
          labels:
            severity: info
          isPaused: false
    - orgId: 1
      name: kubernetes-system-scheduler
      folder: alerts
      interval: 5m
      rules:
        - uid: de5qm5ormic5cc
          title: KubeSchedulerDown
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: absent(up{job="kube-scheduler"} == 1)
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: KubeScheduler has disappeared from Prometheus target discovery.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubeschedulerdown
            summary: Target disappeared from Prometheus target discovery.
          labels:
            severity: critical
          isPaused: false
    - orgId: 1
      name: node-exporter
      folder: alerts
      interval: 5m
      rules:
        - uid: ee5qm5r4t1on4f
          title: NodeBondingDegraded
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: (node_bonding_slaves - node_bonding_active) != 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 5m
          annotations:
            description: Bonding interface {{ $labels.master }} on {{ $labels.instance }} is in degraded state due to one or more slave failures.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodebondingdegraded
            summary: Bonding interface is degraded
          labels:
            severity: warning
          isPaused: false
        - uid: ae5qm5r64hr7ka
          title: NodeCPUHighUsage
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: sum without(mode) (avg without (cpu) (rate(node_cpu_seconds_total{job="node-exporter", mode!="idle"}[2m]))) * 100 > 90
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: |
                CPU usage at {{ $labels.instance }} has been above 90% for the last 15 minutes, is currently at {{ printf "%.2f" $values.QUERY_RESULT.Value }}%.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodecpuhighusage
            summary: High CPU usage.
          labels:
            severity: info
          isPaused: false
        - uid: ae5qm5r66znk0a
          title: NodeClockNotSynchronising
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    min_over_time(node_timex_sync_status{job="node-exporter"}[5m]) == 0
                    and
                    node_timex_maxerror_seconds{job="node-exporter"} >= 16
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: Clock at {{ $labels.instance }} is not synchronising. Ensure NTP is configured on this host.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodeclocknotsynchronising
            summary: Clock not synchronising.
          labels:
            severity: warning
          isPaused: false
        - uid: ae5qm5r66znk1c
          title: NodeClockSkewDetected
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      node_timex_offset_seconds{job="node-exporter"} > 0.05
                    and
                      deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) >= 0
                    )
                    or
                    (
                      node_timex_offset_seconds{job="node-exporter"} < -0.05
                    and
                      deriv(node_timex_offset_seconds{job="node-exporter"}[5m]) <= 0
                    )
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: Clock at {{ $labels.instance }} is out of sync by more than 0.05s. Ensure NTP is configured correctly on this host.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodeclockskewdetected
            summary: Clock skew detected.
          labels:
            severity: warning
          isPaused: false
        - uid: be5qm5r66znk2a
          title: NodeDiskIOSaturation
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}[5m]) > 10
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 30m
          annotations:
            description: |
                Disk IO queue (aqu-sq) is high on {{ $labels.device }} at {{ $labels.instance }}, has been above 10 for the last 30 minutes, is currently at {{ printf "%.2f" $values.QUERY_RESULT.Value }}.
                This symptom might indicate disk saturation.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodediskiosaturation
            summary: Disk IO queue is high.
          labels:
            severity: warning
          isPaused: false
        - uid: be5qm5r66znk3d
          title: NodeFileDescriptorLimit
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 90
                    )
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: File descriptors limit at {{ $labels.instance }} is currently at {{ printf "%.2f" $values.QUERY_RESULT.Value }}%.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefiledescriptorlimit
            summary: Kernel is predicted to exhaust file descriptors limit soon.
          labels:
            severity: critical
          isPaused: false
        - uid: ee5qm5r66znk4d
          title: NodeFileDescriptorLimitWarn
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      node_filefd_allocated{job="node-exporter"} * 100 / node_filefd_maximum{job="node-exporter"} > 70
                    )
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: File descriptors limit at {{ $labels.instance }} is currently at {{ printf "%.2f" $values.QUERY_RESULT.Value }}%.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefiledescriptorlimit
            summary: Kernel is predicted to exhaust file descriptors limit soon.
          labels:
            severity: warning
          isPaused: false
        - uid: be5qm5r69hjwgf
          title: NodeFilesystemAlmostOutOfFilesFivePercent
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 5
                    and
                      node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
                    )
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 1h
          annotations:
            description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $values.QUERY_RESULT.Value }}% available inodes left.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutoffiles
            summary: Filesystem has less than 5% inodes left.
          labels:
            severity: warning
          isPaused: false
        - uid: be5qm5r69hjwha
          title: NodeFilesystemAlmostOutOfFilesThreePercent
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 3
                    and
                      node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
                    )
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 1h
          annotations:
            description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $values.QUERY_RESULT.Value }}% available inodes left.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutoffiles
            summary: Filesystem has less than 3% inodes left.
          labels:
            severity: critical
          isPaused: false
        - uid: ee5qm5r69hjwib
          title: NodeFilesystemAlmostOutOfSpaceFivePercent
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 5
                    and
                      node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
                    )
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 30m
          annotations:
            description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $values.QUERY_RESULT.Value }}% available space left.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace
            summary: Filesystem has less than 5% space left.
          labels:
            severity: warning
          isPaused: false
        - uid: be5qm5r69hjwjc
          title: NodeFilesystemAlmostOutOfSpaceThreePercent
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 3
                    and
                      node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
                    )
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 30m
          annotations:
            description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $values.QUERY_RESULT.Value }}% available space left.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemalmostoutofspace
            summary: Filesystem has less than 3% space left.
          labels:
            severity: critical
          isPaused: false
        - uid: ae5qm5r69hjwke
          title: NodeFilesystemFilesFillingUp
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 40
                    and
                      predict_linear(node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""}[6h], 24*60*60) < 0
                    and
                      node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
                    )
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 1h
          annotations:
            description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $values.QUERY_RESULT.Value }}% available inodes left and is filling up.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemfilesfillingup
            summary: Filesystem is predicted to run out of inodes within the next 24 hours.
          labels:
            severity: warning
          isPaused: false
        - uid: ae5qm5r6bzg8wb
          title: NodeFilesystemFilesFillingUpFast
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_files{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 20
                    and
                      predict_linear(node_filesystem_files_free{job="node-exporter",fstype!="",mountpoint!=""}[6h], 4*60*60) < 0
                    and
                      node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
                    )
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 1h
          annotations:
            description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $values.QUERY_RESULT.Value }}% available inodes left and is filling up fast.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemfilesfillingup
            summary: Filesystem is predicted to run out of inodes within the next 4 hours.
          labels:
            severity: critical
          isPaused: false
        - uid: ee5qm5r6bzg8xe
          title: NodeFilesystemSpaceFillingUp
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 15
                    and
                      predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""}[6h], 24*60*60) < 0
                    and
                      node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
                    )
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 1h
          annotations:
            description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $values.QUERY_RESULT.Value }}% available space left and is filling up.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemspacefillingup
            summary: Filesystem is predicted to run out of space within the next 24 hours.
          labels:
            severity: warning
          isPaused: false
        - uid: de5qm5r6bzg8yb
          title: NodeFilesystemSpaceFillingUpFast
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""} / node_filesystem_size_bytes{job="node-exporter",fstype!="",mountpoint!=""} * 100 < 10
                    and
                      predict_linear(node_filesystem_avail_bytes{job="node-exporter",fstype!="",mountpoint!=""}[6h], 4*60*60) < 0
                    and
                      node_filesystem_readonly{job="node-exporter",fstype!="",mountpoint!=""} == 0
                    )
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 1h
          annotations:
            description: Filesystem on {{ $labels.device }}, mounted on {{ $labels.mountpoint }}, at {{ $labels.instance }} has only {{ printf "%.2f" $values.QUERY_RESULT.Value }}% available space left and is filling up fast.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodefilesystemspacefillingup
            summary: Filesystem is predicted to run out of space within the next 4 hours.
          labels:
            severity: critical
          isPaused: false
        - uid: ee5qm5r6bzg8ze
          title: NodeHighNumberConntrackEntriesUsed
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: (node_nf_conntrack_entries{job="node-exporter"} / node_nf_conntrack_entries_limit) > 0.75
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          annotations:
            description: '{{ $values.QUERY_RESULT.Value | humanizePercentage }} of conntrack entries are used.'
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodehighnumberconntrackentriesused
            summary: Number of conntrack are getting close to the limit.
          labels:
            severity: warning
          isPaused: false
        - uid: fe5qm5r6bzg90d
          title: NodeMemoryHighUtilization
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: 100 - (node_memory_MemAvailable_bytes{job="node-exporter"} / node_memory_MemTotal_bytes{job="node-exporter"} * 100) > 90
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: |
                Memory is filling up at {{ $labels.instance }}, has been above 90% for the last 15 minutes, is currently at {{ printf "%.2f" $values.QUERY_RESULT.Value }}%.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodememoryhighutilization
            summary: Host is running out of memory.
          labels:
            severity: warning
          isPaused: false
        - uid: ee5qm5r6ehclcb
          title: NodeMemoryMajorPagesFaults
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: rate(node_vmstat_pgmajfault{job="node-exporter"}[5m]) > 500
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: |
                Memory major pages are occurring at very high rate at {{ $labels.instance }}, 500 major page faults per second for the last 15 minutes, is currently at {{ printf "%.2f" $values.QUERY_RESULT.Value }}.
                Please check that there is enough memory available at this instance.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodememorymajorpagesfaults
            summary: Memory major page faults are occurring at very high rate.
          labels:
            severity: warning
          isPaused: false
        - uid: de5qm5r6ehclde
          title: NodeNetworkReceiveErrs
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: rate(node_network_receive_errs_total{job="node-exporter"}[2m]) / rate(node_network_receive_packets_total{job="node-exporter"}[2m]) > 0.01
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 1h
          annotations:
            description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf "%.0f" $values.QUERY_RESULT.Value }} receive errors in the last two minutes.'
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworkreceiveerrs
            summary: Network interface is reporting many receive errors.
          labels:
            severity: warning
          isPaused: false
        - uid: ee5qm5r6ehclec
          title: NodeNetworkTransmitErrs
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: rate(node_network_transmit_errs_total{job="node-exporter"}[2m]) / rate(node_network_transmit_packets_total{job="node-exporter"}[2m]) > 0.01
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 1h
          annotations:
            description: '{{ $labels.instance }} interface {{ $labels.device }} has encountered {{ printf "%.0f" $values.QUERY_RESULT.Value }} transmit errors in the last two minutes.'
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodenetworktransmiterrs
            summary: Network interface is reporting many transmit errors.
          labels:
            severity: warning
          isPaused: false
        - uid: de5qm5r6ehclfd
          title: NodeRAIDDegraded
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    node_md_disks_required{job="node-exporter",device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}
                    - ignoring (state) (node_md_disks{state="active",job="node-exporter",device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"})
                    > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: RAID array '{{ $labels.device }}' at {{ $labels.instance }} is in degraded state due to one or more disks failures. Number of spare drives is insufficient to fix issue automatically.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddegraded
            summary: RAID Array is degraded.
          labels:
            severity: critical
          isPaused: false
        - uid: de5qm5r6ehclgb
          title: NodeRAIDDiskFailure
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: node_md_disks{state="failed",job="node-exporter",device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"} > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          annotations:
            description: At least one device in RAID array at {{ $labels.instance }} failed. Array '{{ $labels.device }}' needs attention and possibly a disk swap.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/noderaiddiskfailure
            summary: Failed device in RAID array.
          labels:
            severity: warning
          isPaused: false
        - uid: ae5qm5r6gz8xsa
          title: NodeSystemSaturation
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    node_load1{job="node-exporter"}
                    / count without (cpu, mode) (node_cpu_seconds_total{job="node-exporter", mode="idle"}) > 2
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: |
                System load per core at {{ $labels.instance }} has been above 2 for the last 15 minutes, is currently at {{ printf "%.2f" $values.QUERY_RESULT.Value }}.
                This might indicate this instance resources saturation and can cause it becoming unresponsive.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodesystemsaturation
            summary: System saturated, load per core is very high.
          labels:
            severity: warning
          isPaused: false
        - uid: ae5qm5r6gz8xtc
          title: NodeSystemdServiceFailed
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: node_systemd_unit_state{job="node-exporter", state="failed"} == 1
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 5m
          annotations:
            description: Systemd service {{ $labels.name }} has entered failed state at {{ $labels.instance }}
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodesystemdservicefailed
            summary: Systemd service has entered failed state.
          labels:
            severity: warning
          isPaused: false
        - uid: de5qm5r6gz8xuc
          title: NodeTextFileCollectorScrapeError
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: node_textfile_scrape_error{job="node-exporter"} == 1
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          annotations:
            description: Node Exporter text file collector on {{ $labels.instance }} failed to scrape.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodetextfilecollectorscrapeerror
            summary: Node Exporter text file collector failed to scrape.
          labels:
            severity: warning
          isPaused: false
    - orgId: 1
      name: node-network
      folder: alerts
      interval: 5m
      rules:
        - uid: ee5qm5oyk86psb
          title: NodeNetworkInterfaceFlapping
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: changes(node_network_up{job="node-exporter",device!~"veth.+"}[2m]) > 2
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 2m
          annotations:
            description: Network interface "{{ $labels.device }}" changing its up status often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/general/nodenetworkinterfaceflapping
            summary: Network interface is often changing its status
          labels:
            severity: warning
          isPaused: false
    - orgId: 1
      name: prometheus
      folder: alerts
      interval: 5m
      rules:
        - uid: ae5qm5xc5ytq8a
          title: PrometheusBadConfig
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: max_over_time(prometheus_config_last_reload_successful{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m]) == 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to reload its configuration.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusbadconfig
            summary: Failed Prometheus configuration reload.
          labels:
            severity: critical
          isPaused: false
        - uid: ce5qm5xdrehogd
          title: PrometheusDuplicateTimestamps
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m]) > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf "%.4g" $values.QUERY_RESULT.Value  }} samples/s with different values but duplicated timestamp.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusduplicatetimestamps
            summary: Prometheus is dropping samples with duplicate timestamps.
          labels:
            severity: warning
          isPaused: false
        - uid: ce5qm5xdtwe0wb
          title: PrometheusErrorSendingAlertsToAnyAlertmanager
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    min without (alertmanager) (
                      rate(prometheus_notifications_errors_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring",alertmanager!~``}[5m])
                    /
                      rate(prometheus_notifications_sent_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring",alertmanager!~``}[5m])
                    )
                    * 100
                    > 3
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: '{{ printf "%.1f" $values.QUERY_RESULT.Value }}% minimum errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to any Alertmanager.'
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuserrorsendingalertstoanyalertmanager
            summary: Prometheus encounters more than 3% errors sending alerts to any Alertmanager.
          labels:
            severity: critical
          isPaused: false
        - uid: ee5qm5xdtwe0xe
          title: PrometheusErrorSendingAlertsToSomeAlertmanagers
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      rate(prometheus_notifications_errors_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m])
                    /
                      rate(prometheus_notifications_sent_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m])
                    )
                    * 100
                    > 1
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: '{{ printf "%.1f" $values.QUERY_RESULT.Value }}% errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.alertmanager}}.'
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuserrorsendingalertstosomealertmanagers
            summary: Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager.
          labels:
            severity: warning
          isPaused: false
        - uid: de5qm5xdweadcf
          title: PrometheusHighQueryLoad
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    avg_over_time(prometheus_engine_queries{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m])
                    / max_over_time(prometheus_engine_queries_concurrent_max{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m])
                    > 0.8
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} query API has less than 20% available capacity in its query engine for the last 15 minutes.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheushighqueryload
            summary: Prometheus is reaching its maximum capacity serving concurrent requests.
          labels:
            severity: warning
          isPaused: false
        - uid: ce5qm5xdweadde
          title: PrometheusKubernetesListWatchFailures
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: increase(prometheus_sd_kubernetes_failures_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m]) > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Kubernetes service discovery of Prometheus {{$labels.namespace}}/{{$labels.pod}} is experiencing {{ printf "%.0f" $values.QUERY_RESULT.Value }} failures with LIST/WATCH requests to the Kubernetes API in the last 5 minutes.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuskuberneteslistwatchfailures
            summary: Requests in Kubernetes SD are failing.
          labels:
            severity: warning
          isPaused: false
        - uid: be5qm5xdweadec
          title: PrometheusLabelLimitHit
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: increase(prometheus_target_scrape_pool_exceeded_label_limits_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m]) > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf "%.0f" $values.QUERY_RESULT.Value }} targets because some samples exceeded the configured label_limit, label_name_length_limit or label_value_length_limit.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuslabellimithit
            summary: Prometheus has dropped targets because some scrape configs have exceeded the labels limit.
          labels:
            severity: warning
          isPaused: false
        - uid: ce5qm5xdyw6psd
          title: PrometheusMissingRuleEvaluations
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: increase(prometheus_rule_group_iterations_missed_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m]) > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{ printf "%.0f" $values.QUERY_RESULT.Value }} rule group evaluations in the last 5m.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusmissingruleevaluations
            summary: Prometheus is missing rule evaluations due to slow rule group evaluation.
          labels:
            severity: warning
          isPaused: false
        - uid: ae5qm5xdyw6pte
          title: PrometheusNotConnectedToAlertmanagers
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: max_over_time(prometheus_notifications_alertmanagers_discovered{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m]) < 1
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not connected to any Alertmanagers.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotconnectedtoalertmanagers
            summary: Prometheus is not connected to any Alertmanagers.
          labels:
            severity: warning
          isPaused: false
        - uid: ee5qm5xdyw6pua
          title: PrometheusNotIngestingSamples
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      sum without(type) (rate(prometheus_tsdb_head_samples_appended_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m])) <= 0
                    and
                      (
                        sum without(scrape_job) (prometheus_target_metadata_cache_entries{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}) > 0
                      or
                        sum without(rule_group) (prometheus_rule_group_rules{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}) > 0
                      )
                    )
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting samples.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotingestingsamples
            summary: Prometheus is not ingesting samples.
          labels:
            severity: warning
          isPaused: false
        - uid: ae5qm5xe1e328f
          title: PrometheusNotificationQueueRunningFull
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      predict_linear(prometheus_notifications_queue_length{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m], 60 * 30)
                    >
                      min_over_time(prometheus_notifications_queue_capacity{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m])
                    )
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Alert notification queue of Prometheus {{$labels.namespace}}/{{$labels.pod}} is running full.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotificationqueuerunningfull
            summary: Prometheus alert notification queue predicted to run full in less than 30m.
          labels:
            severity: warning
          isPaused: false
        - uid: de5qm5xe1e329d
          title: PrometheusOutOfOrderTimestamps
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: rate(prometheus_target_scrapes_sample_out_of_order_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m]) > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf "%.4g" $values.QUERY_RESULT.Value  }} samples/s with timestamps arriving out of order.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusoutofordertimestamps
            summary: Prometheus drops samples with out-of-order timestamps.
          labels:
            severity: warning
          isPaused: false
        - uid: ce5qm5xe1e32ae
          title: PrometheusRemoteStorageFailures
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      (rate(prometheus_remote_storage_failed_samples_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m]))
                    /
                      (
                        (rate(prometheus_remote_storage_failed_samples_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m]))
                      +
                        (rate(prometheus_remote_storage_succeeded_samples_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m]) or rate(prometheus_remote_storage_samples_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m]))
                      )
                    )
                    * 100
                    > 1
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} failed to send {{ printf "%.1f" $values.QUERY_RESULT.Value }}% of the samples to {{ $labels.remote_name}}:{{ $labels.url }}
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotestoragefailures
            summary: Prometheus fails to send samples to remote storage.
          labels:
            severity: critical
          isPaused: false
        - uid: ee5qm5xe1e32be
          title: PrometheusRemoteWriteBehind
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m])
                    - ignoring(remote_name, url) group_right
                      max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m])
                    )
                    > 120
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write is {{ printf "%.1f" $values.QUERY_RESULT.Value }}s behind for {{ $labels.remote_name}}:{{ $labels.url }}.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotewritebehind
            summary: Prometheus remote write is behind.
          labels:
            severity: critical
          isPaused: false
        - uid: de5qm5xe1e32cf
          title: PrometheusRemoteWriteDesiredShards
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (
                      max_over_time(prometheus_remote_storage_shards_desired{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m])
                    >
                      max_over_time(prometheus_remote_storage_shards_max{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m])
                    )
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write desired shards calculation wants to run {{ $values.QUERY_RESULT.Value }} shards for queue {{ $labels.remote_name}}:{{ $labels.url }}, which is more than the max of {{ printf `prometheus_remote_storage_shards_max{instance="%s",job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}` $labels.instance | query | first | value }}.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotewritedesiredshards
            summary: Prometheus remote write desired shards calculation wants to run more than configured max shards.
          labels:
            severity: warning
          isPaused: false
        - uid: ee5qm5xe3vzeoa
          title: PrometheusRuleFailures
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: increase(prometheus_rule_evaluation_failures_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m]) > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to evaluate {{ printf "%.0f" $values.QUERY_RESULT.Value }} rules in the last 5m.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusrulefailures
            summary: Prometheus is failing rule evaluations.
          labels:
            severity: critical
          isPaused: false
        - uid: ae5qm5xe3vzepc
          title: PrometheusSDRefreshFailure
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: increase(prometheus_sd_refresh_failures_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[10m]) > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 20m
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to refresh SD with mechanism {{$labels.mechanism}}.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheussdrefreshfailure
            summary: Failed Prometheus SD refresh.
          labels:
            severity: warning
          isPaused: false
        - uid: ee5qm5xe3vzeqd
          title: PrometheusScrapeBodySizeLimitHit
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: increase(prometheus_target_scrapes_exceeded_body_size_limit_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m]) > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{ printf "%.0f" $values.QUERY_RESULT.Value }} scrapes in the last 5m because some targets exceeded the configured body_size_limit.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusscrapebodysizelimithit
            summary: Prometheus has dropped some targets that exceeded body size limit.
          labels:
            severity: warning
          isPaused: false
        - uid: de5qm5xe3vzerd
          title: PrometheusScrapeSampleLimitHit
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: increase(prometheus_target_scrapes_exceeded_sample_limit_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m]) > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed {{ printf "%.0f" $values.QUERY_RESULT.Value }} scrapes in the last 5m because some targets exceeded the configured sample_limit.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusscrapesamplelimithit
            summary: Prometheus has failed scrapes that have exceeded the configured sample limit.
          labels:
            severity: warning
          isPaused: false
        - uid: fe5qm5xe3vzesb
          title: PrometheusTSDBCompactionsFailing
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: increase(prometheus_tsdb_compactions_failed_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[3h]) > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 4h
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$values.QUERY_RESULT.Value | humanize}} compaction failures over the last 3h.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustsdbcompactionsfailing
            summary: Prometheus has issues compacting blocks.
          labels:
            severity: warning
          isPaused: false
        - uid: fe5qm5xe6dvr4c
          title: PrometheusTSDBReloadsFailing
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: increase(prometheus_tsdb_reloads_failures_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[3h]) > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 4h
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$values.QUERY_RESULT.Value | humanize}} reload failures over the last 3h.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustsdbreloadsfailing
            summary: Prometheus has issues reloading blocks from disk.
          labels:
            severity: warning
          isPaused: false
        - uid: ee5qm5xe6dvr5c
          title: PrometheusTargetLimitHit
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: increase(prometheus_target_scrape_pool_exceeded_target_limit_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[5m]) > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf "%.0f" $values.QUERY_RESULT.Value }} targets because the number of targets exceeded the configured target_limit.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustargetlimithit
            summary: Prometheus has dropped targets because some scrape configs have exceeded the targets limit.
          labels:
            severity: warning
          isPaused: false
        - uid: ae5qm5xe6dvr6b
          title: PrometheusTargetSyncFailure
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: increase(prometheus_target_sync_failed_total{job="prometheus-kube-prometheus-prometheus",namespace="monitoring"}[30m]) > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 5m
          annotations:
            description: '{{ printf "%.0f" $values.QUERY_RESULT.Value }} targets in Prometheus {{$labels.namespace}}/{{$labels.pod}} have failed to sync because invalid configuration was supplied.'
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustargetsyncfailure
            summary: Prometheus has failed to sync targets.
          labels:
            severity: critical
          isPaused: false
    - orgId: 1
      name: prometheus-operator
      folder: alerts
      interval: 5m
      rules:
        - uid: ce5qm5vwa2r5sc
          title: PrometheusOperatorListErrors
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (sum by (cluster,controller,namespace) (rate(prometheus_operator_list_operations_failed_total{job="prometheus-kube-prometheus-operator",namespace="monitoring"}[10m]))
                    / sum by (cluster,controller,namespace) (rate(prometheus_operator_list_operations_total{job="prometheus-kube-prometheus-operator",namespace="monitoring"}[10m])))
                    > 0.4
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Errors while performing List operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorlisterrors
            summary: Errors while performing list operations in controller.
          labels:
            severity: warning
          isPaused: false
        - uid: ae5qm5vwa2r5te
          title: PrometheusOperatorNodeLookupErrors
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: rate(prometheus_operator_node_address_lookup_errors_total{job="prometheus-kube-prometheus-operator",namespace="monitoring"}[5m]) > 0.1
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: Errors while reconciling Prometheus in {{ $labels.namespace }} Namespace.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatornodelookuperrors
            summary: Errors while reconciling Prometheus.
          labels:
            severity: warning
          isPaused: false
        - uid: be5qm5vwhkg74c
          title: PrometheusOperatorNotReady
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: min by (cluster,controller,namespace) (max_over_time(prometheus_operator_ready{job="prometheus-kube-prometheus-operator",namespace="monitoring"}[5m]) == 0)
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 5m
          annotations:
            description: Prometheus operator in {{ $labels.namespace }} namespace isn't ready to reconcile {{ $labels.controller }} resources.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatornotready
            summary: Prometheus operator not ready
          labels:
            severity: warning
          isPaused: false
        - uid: de5qm5vwhkg75d
          title: PrometheusOperatorReconcileErrors
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (sum by (cluster,controller,namespace) (rate(prometheus_operator_reconcile_errors_total{job="prometheus-kube-prometheus-operator",namespace="monitoring"}[5m])))
                    / (sum by (cluster,controller,namespace) (rate(prometheus_operator_reconcile_operations_total{job="prometheus-kube-prometheus-operator",namespace="monitoring"}[5m])))
                    > 0.1
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: '{{ $values.QUERY_RESULT.Value | humanizePercentage }} of reconciling operations failed for {{ $labels.controller }} controller in {{ $labels.namespace }} namespace.'
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorreconcileerrors
            summary: Errors while reconciling objects.
          labels:
            severity: warning
          isPaused: false
        - uid: de5qm5vwk2cjkd
          title: PrometheusOperatorRejectedResources
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: min_over_time(prometheus_operator_managed_resources{state="rejected",job="prometheus-kube-prometheus-operator",namespace="monitoring"}[5m]) > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 5m
          annotations:
            description: Prometheus operator in {{ $labels.namespace }} namespace rejected {{ printf "%0.0f" $values.QUERY_RESULT.Value }} {{ $labels.controller }}/{{ $labels.resource }} resources.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorrejectedresources
            summary: Resources rejected by Prometheus operator
          labels:
            severity: warning
          isPaused: false
        - uid: ce5qm5vwmk8w0e
          title: PrometheusOperatorStatusUpdateErrors
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (sum by (cluster,controller,namespace) (rate(prometheus_operator_status_update_errors_total{job="prometheus-kube-prometheus-operator",namespace="monitoring"}[5m])))
                    / (sum by (cluster,controller,namespace) (rate(prometheus_operator_status_update_operations_total{job="prometheus-kube-prometheus-operator",namespace="monitoring"}[5m])))
                    > 0.1
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: '{{ $values.QUERY_RESULT.Value | humanizePercentage }} of status update operations failed for {{ $labels.controller }} controller in {{ $labels.namespace }} namespace.'
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorstatusupdateerrors
            summary: Errors while updating objects status.
          labels:
            severity: warning
          isPaused: false
        - uid: ce5qm5vwp258gd
          title: PrometheusOperatorSyncFailed
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: min_over_time(prometheus_operator_syncs{status="failed",job="prometheus-kube-prometheus-operator",namespace="monitoring"}[5m]) > 0
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 10m
          annotations:
            description: Controller {{ $labels.controller }} in {{ $labels.namespace }} namespace fails to reconcile {{ $values.QUERY_RESULT.Value }} objects.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorsyncfailed
            summary: Last controller reconciliation failed
          labels:
            severity: warning
          isPaused: false
        - uid: ee5qm5vwrk1kwa
          title: PrometheusOperatorWatchErrors
          condition: ALERTCONDITION
          data:
            - refId: QUERY
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: PBFA97CFB590B2093
              model:
                editorMode: code
                expr: |-
                    (sum by (cluster,controller,namespace) (rate(prometheus_operator_watch_operations_failed_total{job="prometheus-kube-prometheus-operator",namespace="monitoring"}[5m]))
                    / sum by (cluster,controller,namespace) (rate(prometheus_operator_watch_operations_total{job="prometheus-kube-prometheus-operator",namespace="monitoring"}[5m])))
                    > 0.4
                intervalMs: 1000
                maxDataPoints: 43200
                refId: QUERY
            - refId: QUERY_RESULT
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params: []
                      reducer:
                        params: []
                        type: avg
                      type: query
                datasource:
                    name: Expression
                    type: __expr__
                    uid: __expr__
                expression: QUERY
                intervalMs: 1000
                maxDataPoints: 43200
                reducer: last
                refId: QUERY_RESULT
                type: reduce
            - refId: ALERTCONDITION
              relativeTimeRange:
                from: 600
                to: 0
              datasourceUid: __expr__
              model:
                conditions:
                    - evaluator:
                        params:
                            - 0
                        type: gt
                      operator:
                        type: and
                      query:
                        params:
                            - QUERY_RESULT
                      reducer:
                        params: []
                        type: last
                      type: query
                datasource:
                    type: __expr__
                    uid: __expr__
                expression: QUERY_RESULT
                hide: false
                intervalMs: 1000
                maxDataPoints: 43200
                refId: ALERTCONDITION
                type: threshold
          noDataState: OK
          execErrState: Error
          for: 15m
          annotations:
            description: Errors while performing watch operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.
            runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorwatcherrors
            summary: Errors while performing watch operations in controller.
          labels:
            severity: warning
          isPaused: false
